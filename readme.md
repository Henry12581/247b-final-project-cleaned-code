This project evaluates different LLMs by generating and executing Z3 code. The pipeline supports:

DeepSeek API (using DeepSeek’s provided API)

LM Studio pipeline (running models locally)

The overall goal is to ensure each model produces Z3-only code, clean it, execute it, and score the logical correctness.

Project Structure
problems/

Contains all the logic questions.
Each question will be used as input for the LLMs.

llm_output/

Contains all original text outputs generated by each model.

Inside each model’s output folder:

aio_scoring/
All-In-One scoring. Produces the final evaluation score.

graphing/
make_graph.py creates graphs by raw score (e.g., 17/20) or by percentage (e.g., 87%).

llm_cleaning/
Python code that uses an LLM to clean and debug raw Z3 code generated by low-parameter models.

test.exe
Executes the cleaned Z3 code produced by the LLM and outputs the score.

Meaning of _z3 files

_z3 in the filename means:

The file contains only Z3 code.

No explanation or natural language text.

Output Files for Each LLM

For every model, you should see four files:

llama_3.1_8b_z3
Raw output generated by the model (Z3 code only).

llama_3.1_8b_z3_cleaned_by_api
Debugged and cleaned version of the Z3 code produced by the cleaning pipeline or API.

llama_3.1_8b_z3_extracted_llm_answers
Execution results from test.exe.
Contains the answers produced for each question.

llama_3.1_8b_z3_logic_score
Final evaluation score for that model.
Includes incorrect answers for debugging.
