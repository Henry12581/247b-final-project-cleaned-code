## ðŸ“š Project Summary: LLM Evaluation via Z3 Code Generation

This project establishes a pipeline to evaluate various **Large Language Models (LLMs)** based on their ability to generate logically correct **Z3 SMT solver code**. The evaluation process includes generating Z3 code, cleaning it for execution, running the code, and scoring the logical correctness of the results.

---

### ðŸ› ï¸ Supported LLM Integration Methods

The pipeline can integrate with models using the following methods:

* **DeepSeek API:** Utilizes the API provided by DeepSeek.
* **LM Studio Pipeline:** Runs models locally using the LM Studio environment.

---

### ðŸ“‚ Project Structure & Key Components

| Folder/File | Purpose |
| :--- | :--- |
| `problems/` | Contains all the **logic questions** used as input prompts for the LLMs. |
| `llm_output/` | Parent directory for all **original text outputs** generated by each model. |
| `aio_scoring/` | Performs the **All-In-One scoring** to produce the final evaluation score. |
| `graphing/` | Contains `make_graph.py` to create evaluation **graphs** based on raw scores (e.g., 17/20) or percentages (e.g., 87%). |
| `llm_cleaning/` | Python code that uses an **LLM** (via API) to **clean and debug** the raw Z3 code, especially for low-parameter models. |
| `test.exe` | **Executes** the cleaned Z3 code and outputs the results (answers). |

---

### ðŸ“ Meaning of `_z3` in Filenames

The suffix `_z3` indicates that the file strictly adheres to the following conditions:

> The file contains **only Z3 code** with **no explanation or natural language text**.

---

### ðŸ“Š Output Files for Each LLM

For every model evaluated, four specific files are generated to track the process from raw output to final score (using `llama_3.1_8b` as an example):

| Output File Name | Description | Stage |
| :--- | :--- | :--- |
| `llama_3.1_8b_z3` | The **raw Z3 code output** generated directly by the LLM. | **Generation** |
| `llama_3.1_8b_z3_cleaned_by_api` | The **debugged and cleaned** version of the Z3 code, processed by the cleaning pipeline or API. | **Cleaning** |
| `llama_3.1_8b_z3_extracted_llm_answers` | The **execution results** from running the cleaned Z3 code via `test.exe`. Contains the computed answers. | **Execution** |
| `llama_3.1_8b_z3_logic_score` | The **final evaluation score** for the model, including incorrect answers for debugging purposes. | **Scoring** |
